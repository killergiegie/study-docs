# vLLM éƒ¨ç½²ä¸Žä½¿ç”¨

## 1. é…ç½®çŽ¯å¢ƒ

### 1.1 vLLM 0.9.0.1 ç‰ˆæœ¬ä»Žæºç æž„å»º CPU ç‰ˆ

1. å®‰è£… Pytorch 2.6 + cpu
```bash
# CPU only
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu
# æ£€æŸ¥å®‰è£…æ˜¯å¦æˆåŠŸ
python -c "import torch; print(torch.__version__, torch.version.cuda, torch.cuda.is_available())"
```

2. ä¸‹è½½ vLLM 0.9.0.1 ç‰ˆæœ¬æºç ï¼ˆæœ€å¥½æ˜¯ git cloneï¼Œéœ€è¦```.git```ç›®å½•ï¼‰ï¼Œè®¾ç½®ä½¿ç”¨æœ¬åœ°çš„pytorch
```bash
python use_existing_torch.py        
```

3. ä¸‹è½½ä¾èµ–åº“
```bash
pip install -r requirements/build.txt requirements/common.txt requirements/cpu.txt
```

4. ä¿®æ”¹æ–‡ä»¶

ä¿®æ”¹ ```pyproject.toml``` æ–‡ä»¶ä¸­çš„
license = "Apache-2.0"
æ”¹ä¸º
license = { text = "Apache-2.0" }
åˆ é™¤ ```pyproject.tom```l æ–‡ä»¶ä¸­çš„è¿™ä¸€è¡Œ
license-files = ["LICENSE"]

5. ä¸‹è½½å®‰è£…ä¾èµ–åº“ numaï¼Œå¹¶åŠ å…¥è·¯å¾„
```bash
export NUMACTL_DIR=$HOME/local/numactl
export C_INCLUDE_PATH=$NUMACTL_DIR/include:$C_INCLUDE_PATH
export LIBRARY_PATH=$NUMACTL_DIR/lib:$LIBRARY_PATH
export LD_LIBRARY_PATH=$NUMACTL_DIR/lib:$LD_LIBRARY_PATH
export CPATH=$NUMACTL_DIR/include:$CPATH
```

6. ç¼–è¯‘vLLM
```bash
VLLM_TARGET_DEVICE=cpu python setup.py install
```

7. ä½¿ç”¨vLLM
```bash
export VLLM_CPU_OMP_THREADS_BIND=0-15
export OMP_NUM_THREADS=16
export VLLM_CPU_KVCACHE_SPACE=40  # å•ä½æ˜¯ MB
```

ç¼–å†™pythonè„šæœ¬
```python
import os
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

def main():
    os.environ['VLLM_TARGET_DEVICE'] = 'cpu'
    os.environ['VLLM_USE_LEGACY_ENGINE'] = 'true'

    model_dir = "/home2/ghy/.cache/modelscope/hub/models/LLM-Research/gemma-3-12b-it"

    tokenizer = AutoTokenizer.from_pretrained(
        model_dir,
        local_files_only=True,
        trust_remote_code=True,
    )

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "å¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„ï¼Ÿ"}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    llm = LLM(
        model=model_dir,
        tensor_parallel_size=1,
        dtype="bfloat16",  # CPU å¿…é¡»ç”¨ float32
        trust_remote_code=True,
        # model_impl='transformers',
        device="cpu",
        max_model_len=2048,
    )

    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.8,
        repetition_penalty=1.05,
        max_tokens=512,
    )

    outputs = llm.generate([prompt], sampling_params, stream=True)

    print("\n=== Prompt ===")
    print(prompt)
    print("\n=== Output ===")

    for request_output in outputs:
        # æ¯æ¬¡è¿½åŠ çš„æ˜¯æ–°çš„ tokensï¼ˆå¢žé‡ï¼‰
        print(request_output.outputs[0].text, end='', flush=True)


# ðŸ‘‡ å…³é”®ï¼šå¿…é¡»åŠ è¿™å¥
if __name__ == "__main__":
    main()

```


### 1.2 vLLM 0.9.0.1 ç‰ˆæœ¬ä»Žæºç æž„å»º GPU + CUDA-11.8 ç‰ˆ

1. å…ˆå®‰è£… CUDA 11.8
```bash
cd ~/local/src
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sh cuda_11.8.0_520.61.05_linux.run --toolkit --silent --toolkitpath=$HOME/local/cuda-11.8
# æ£€æŸ¥å®‰è£…æ˜¯å¦æˆåŠŸ
nvcc --version
```
ä¿®æ”¹çŽ¯å¢ƒé…ç½®æ–‡ä»¶ï¼ŒåŠ å…¥cudaçš„è·¯å¾„

2. å®‰è£… Pytorch 2.6 + cu118
```bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# æ£€æŸ¥å®‰è£…æ˜¯å¦æˆåŠŸ
python -c "import torch; print(torch.__version__, torch.version.cuda, torch.cuda.is_available())"
```

3. ä¸‹è½½ vLLM 0.9.0.1 ç‰ˆæœ¬æºç ï¼ˆæœ€å¥½æ˜¯ git cloneï¼Œéœ€è¦```.git```ç›®å½•ï¼‰ï¼Œè®¾ç½®ä½¿ç”¨æœ¬åœ°çš„pytorch
```bash
python use_existing_torch.py        
```

4. ä¸‹è½½ä¾èµ–åº“
```bash
pip install -r requirements/build.txt requirements/common.txt
```

5. ä¿®æ”¹æ–‡ä»¶
ä¿®æ”¹ ```pyproject.toml``` æ–‡ä»¶ä¸­çš„
license = "Apache-2.0"
æ”¹ä¸º
license = { text = "Apache-2.0" }
åˆ é™¤ ```pyproject.tom```l æ–‡ä»¶ä¸­çš„è¿™ä¸€è¡Œ
license-files = ["LICENSE"]

6. æ‰‹åŠ¨ä¸‹è½½ä¸¤ä¸ªgithubåŒ…ï¼Œå¹¶ä¿®æ”¹cmakeæ–‡ä»¶
ä¸‹è½½cutlassä¸ŽFlashMLAï¼Œè§£åŽ‹è‡³ ```vllm-0.9.0.1/vllm/third_party/``` ç›®å½•ä¸‹
æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶ ```vllm-0.9.0.1/cmake/external_projects/flashmla.cmake```:
```GIT_REPOSITORY https://github.com/vllm-project/FlashMLA.git```
æ”¹ä¸º
```SOURCE_DIR ${CMAKE_SOURCE_DIR}/vllm/third_party/FlashMLA```
åŒç†ä¿®æ”¹ ```vllm-0.9.0.1/CMakeLists.txt``` æ–‡ä»¶

7. ç¼–è¯‘vLLM
```bash
# VLLM_TARGET_DEVICE=cpu python setup.py install
pip install --no-build-isolation -e .
```

8. ä½¿ç”¨vLLM
```bash
pip install xformers==0.0.25
```

ç¼–å†™pythonè„šæœ¬

```python
from vllm import LLM, SamplingParams

llm = LLM(model="../gemma-3-12b-it-Q4_K_M.gguf")  # æ›¿æ¢ä¸ºä½ çš„æ¨¡åž‹è·¯å¾„

prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ç›¸å¯¹è®ºçš„åŸºæœ¬æ€æƒ³ã€‚"
sampling_params = SamplingParams(max_tokens=128, temperature=0.7)

outputs = llm.generate(prompt, sampling_params)
print(outputs[0].outputs[0].text)
```

> ç”±äºŽæµ‹è¯•çš„æ¨¡åž‹ä¸æ”¯æŒ FP16 æœºæ¢°èƒ½æŽ¨ç†ï¼Œä¸”V100ä¸æ”¯æŒ BF16ï¼Œæ‰€ä»¥æ²¡è¾“å‡º

## 2. vLLM éƒ¨ç½²æ¨¡åž‹(GPU)

é¦–å…ˆéœ€è¦ä»Ž ModelScope ä¸‹è½½æ¨¡åž‹æ–‡ä»¶
```bash
modelscope download --model Qwen/Qwen2.5-VL-7B-Instruct --local_dir ./qwen2_5
modelscope download --model LLM-Research/gemma-3-12b-it --local_dir ./gemma-3
```

### 2.1 vLLM-Serverç«¯
```bash
# å¼ºåˆ¶åªä½¿ç”¨ GPU1
export CUDA_VISIBLE_DEVICES=0,1
# ä½¿ç”¨ ModelScope çš„æ¨¡åž‹
export VLLM_USE_MODELSCOPE=True
# å¯ç”¨ â€œå¯æ‰©å±•æ®µå¼â€ æ˜¾å­˜åˆ†é…ç­–ç•¥
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python -m vllm.entrypoints.openai.api_server \
  --model /home2/ghy/.cache/modelscope/hub/models/LLM-Research/gemma-3-12b-it-fp32 \
  --tokenizer /home2/ghy/.cache/modelscope/hub/models/LLM-Research/gemma-3-12b-it-fp32 \
  --dtype float32 \
  --port 8000 \
  --trust-remote-code \
  --served-model-name gemma-3-12b-it \
  --gpu-memory-utilization 0.95 \
  --max-model-len 8192 \
  --max-num-seqs 1 \
  --enforce-eager

```
--chat-template /home2/ghy/.cache/modelscope/hub/models/LLM-Research/gemma-3-12b-it/chat_template_fixed.json \
```bash
--chat-template # æŒ‡å®šå¯¹è¯æ¨¡æ¿
--gpu-memory-utilization # æŒ‡å®š GPU æ˜¾å­˜çš„æœ€å¤§å ç”¨çŽ‡
--max-num-seqs # æŒ‡å®šå¹¶å‘æ•°é‡
--mm-embed-counts image:256 # æŒ‡å®šå›¾åƒenbeddingåŽçš„å¤§å°
--enforce-eager # ç¡®ä¿æ¨¡åž‹åŠ è½½åŽç«‹å³æ‰§è¡Œåˆå§‹åŒ–
```

### 2.2 vLLM-Clientç«¯
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma-3-12b-it",
    "messages": [
      {
        "role": "system",
        "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¹äºŽåŠ©äººçš„æ•°å­¦åŠ©æ‰‹"}]
      },
      {
        "role": "user",
        "content": [{"type": "text", "text": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä¸‰è§’å‡½æ•°"}]
      }
    ],
    "temperature": 0.7,
    "max_tokens": 512
  }'


```

### 2.3 å›¾ç‰‡è®¿é—®ç«¯
```bash
python3 -m http.server 8008
```

## 3. vLLM éƒ¨ç½²é‡åŒ–æ¨¡åž‹ï¼ˆCPUï¼‰

```bash
export VLLM_CPU_OMP_THREADS_BIND=0-23
export OMP_NUM_THREADS=24
export VLLM_CPU_KVCACHE_SPACE=40  # å•ä½æ˜¯ MB

export VLLM_USE_LEGACY_ENGINE=true
export VLLM_USE_V1=0

LD_PRELOAD=/usr/local/gcc-13.2.0/lib64/libstdc++.so.6 VLLM_USE_LEGACY_ENGINE=true python gemma_server.py

# bf16
LD_PRELOAD=/usr/local/gcc-13.2.0/lib64/libstdc++.so.6 VLLM_USE_LEGACY_ENGINE=true python -m vllm.entrypoints.openai.api_server \
  --model /home2/ghy/.cache/modelscope/hub/models/LLM-Research/gemma-3-12b-it \
  --tokenizer /home2/ghy/.cache/modelscope/hub/models/LLM-Research/gemma-3-12b-it \
  --dtype bfloat16 \
  --device cpu \
  --port 8000 \
  --trust-remote-code \
  --served-model-name gemma-3-12b-it \
  --max-model-len 2048 \
  --max-num-seqs 1 \
  --enforce-eager

# w4a16
LD_PRELOAD=/usr/local/gcc-13.2.0/lib64/libstdc++.so.6 VLLM_USE_LEGACY_ENGINE=true python -m vllm.entrypoints.openai.api_server \
  --model /data/home2/ghy/.cache/modelscope/hub/models/nm-testing/gemma-3-12b-it-quantized___w4a16 \
  --tokenizer /data/home2/ghy/.cache/modelscope/hub/models/nm-testing/gemma-3-12b-it-quantized___w4a16 \
  --dtype bfloat16 \
  --device cpu \
  --port 8000 \
  --trust-remote-code \
  --served-model-name gemma-3-12b-it \
  --max-model-len 2048 \
  --max-num-seqs 1 \
  --enforce-eager

# w8a8
LD_PRELOAD=/usr/local/gcc-13.2.0/lib64/libstdc++.so.6 VLLM_USE_LEGACY_ENGINE=true python -m vllm.entrypoints.openai.api_server \
  --model /data/home2/ghy/.cache/modelscope/hub/models/nm-testing/gemma-3-12b-it-quantized___w8a8 \
  --tokenizer /data/home2/ghy/.cache/modelscope/hub/models/nm-testing/gemma-3-12b-it-quantized___w8a8 \
  --dtype bfloat16 \
  --device cpu \
  --port 8000 \
  --trust-remote-code \
  --served-model-name gemma-3-12b-it \
  --max-model-len 2048 \
  --max-num-seqs 1 \
  --load-format bitsandbytes \
  --quantization bitsandbytes \
  --enforce-eager

# bnb
LD_PRELOAD=/usr/local/gcc-13.2.0/lib64/libstdc++.so.6 VLLM_USE_LEGACY_ENGINE=true python -m vllm.entrypoints.openai.api_server \
  --model /data/home2/ghy/.cache/modelscope/hub/models/AIDC-AI/Ovis1___6-Gemma2-9B-GPTQ-Int4/ \
  --tokenizer /data/home2/ghy/.cache/modelscope/hub/models/AIDC-AI/Ovis1___6-Gemma2-9B-GPTQ-Int4/ \
  --dtype bfloat16 \
  --device cpu \
  --port 8000 \
  --trust-remote-code \
  --served-model-name gemma-3-12b-it \
  --max-model-len 2048 \
  --max-num-seqs 1 \
  --quantization gptq \
  --add_bos_token=True \
  --enforce-eager
```

## 3. å¤‡ä»½ conda çŽ¯å¢ƒ
```conda
conda env export > ~/vllm_cpu_env_full.yml
conda env create -n vllm_cpu_copy -f vllm_cpu_env_full.yml
```

